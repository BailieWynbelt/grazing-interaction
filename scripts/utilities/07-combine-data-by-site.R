# clear the R environment
rm(list=ls(all=TRUE))

source(paste0(getwd(), "/environment.R"))

print(currentwd)

# load in the required libraries
source(paste0(currentwd, "/packages.R"))

source(paste0(currentwd, "/functions.R"))

# set working directory to location of recombined csv files
# file.path() is system agnostic (i.e. works on Mac/PC/Linux)
path_to_recombined_csv_files <- file.path(currentwd, "data", "xlsm", "csv", "recombined")

# list csv files that were recombined in the previous step
# we want to recombine these files into a single csv file for each year
csv_file_list <- list.files(path_to_recombined_csv_files, pattern = ".csv")

# convert to a tibble for pretty printing
csv_files_tibble <- tibble(csv_file_list)

# rename the first column to "path" to be more descriptive of what that column represents
names(csv_files_tibble)[names(csv_files_tibble) == "csv_file_list"] <- "path"

# create new columns using the name of each file as metadata
# we can use these new columns to filter and handle the data
csv_files_tibble_separated <- separate(csv_files_tibble, path, 
                                   into = c("sitecode",
                                            "deploydate",
                                            "collectdate",
                                            "subjects",
                                            "all",
                                            "chunks"),
                                   sep = "_", 
                                   remove = FALSE)

# create a year column that we can use to group by year
csv_files_tibble_separated <- csv_files_tibble_separated %>% dplyr::mutate(year = str_sub(collectdate, -4))

# unique sites
sitecodes_in_csv_files <- unique(csv_files_tibble_separated$sitecode)

# print the site codes in console
unique(csv_files_tibble_separated$sitecode)

# the data frame generated by list.files has the csv files for all sites
# but we want to combine data by site and write out a file for each site
# we can use dplyr to group by the site code
grouped_site_dataframes <- csv_files_tibble_separated %>% group_by(sitecode, year)

# and then use the group split to create a list
# each element corresponds to a site data frame
grouped_site_list <- dplyr::group_split(grouped_site_dataframes)

# this prints the first site in the list
grouped_site_list[[1]]

# get data used to group each data frame
group_dataframe <- group_data(grouped_site_dataframes)

# create a new "names" column that we can use to name elements in the list of data frames
group_names_df <- unite(group_dataframe, year, sitecode, col = name, sep = "_", remove = FALSE)

# set the names of the elements in the list by using the site codes
# this will help us access components by using their names (instead of index)
# e.g., grouped_site_list$BKS instead of grouped_site_list$1
names(grouped_site_list) <- group_names_df$name

# read in the data from each recombined csv file and bind rows together
for (i in 1:length(grouped_site_list)) {
  data <-
    file.path(path_to_recombined_csv_files, grouped_site_list[[i]]$path) %>% 
    lapply(readr::read_csv) %>% 
    dplyr::bind_rows()
  
  # convert the "DateTime" column into a standardized format
  data$DateTime <- lubridate::ymd_hms(data$DateTime)
  
  # construct a file name that uses the year from the data and the site code 
  filename <- paste0(group_names_df$name[i], ".csv")
  
  # write out the data as a csv file using the file name convention
  write_excel_csv(data, file.path(currentwd, "data", "photo", filename))
}

# get the current system time to notify when the script is completed
# note that this defaults to UTC (aka Greenwich Mean Time)
system_time <- Sys.time()

# convert into the correct timezone for your locale (mine is Arizona so we follow Mountain Standard)
attr(system_time,"tzone") <- "MST"

msg_body <- paste("07-combine-data-by-site.R", "run on folder", collection_folder, "completed at", system_time, sep = " ")

RPushbullet::pbPost(type = "note", title = "Script Completed", body = msg_body)
